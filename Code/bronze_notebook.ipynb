{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BqC4M27tDxQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUG3DLVKlPT",
        "outputId": "bb197154-7044-4bf3-d4e2-bfaf3b1bb5cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last processed season from checkpoint: 2011\n",
            "Processing seasons: [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
            "\n",
            "Processing season 2012\n",
            "Fetching season 2012\n",
            "Saved 24 races for season 2012\n",
            "\n",
            "Processing season 2013\n",
            "Fetching season 2013\n",
            "Saved 23 races for season 2013\n",
            "\n",
            "Processing season 2014\n",
            "Fetching season 2014\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 23 races for season 2014\n",
            "\n",
            "Processing season 2015\n",
            "Fetching season 2015\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 22 races for season 2015\n",
            "\n",
            "Processing season 2016\n",
            "Fetching season 2016\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 25 races for season 2016\n",
            "\n",
            "Processing season 2017\n",
            "Fetching season 2017\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 20 races for season 2017\n",
            "\n",
            "Processing season 2018\n",
            "Fetching season 2018\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 3s\n",
            "Saved 21 races for season 2018\n",
            "\n",
            "Processing season 2019\n",
            "Fetching season 2019\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 21 races for season 2019\n",
            "\n",
            "Processing season 2020\n",
            "Fetching season 2020\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 3s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 17 races for season 2020\n",
            "\n",
            "Processing season 2021\n",
            "Fetching season 2021\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 22 races for season 2021\n",
            "\n",
            "Processing season 2022\n",
            "Fetching season 2022\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 22 races for season 2022\n",
            "\n",
            "Processing season 2023\n",
            "Fetching season 2023\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 3s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 22 races for season 2023\n",
            "\n",
            "Processing season 2024\n",
            "Fetching season 2024\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 28 races for season 2024\n",
            "\n",
            "Processing season 2025\n",
            "Force refreshing current season 2025\n",
            "Fetching season 2025\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Rate limited. Waiting 2s\n",
            "Saved 6 races for season 2025\n",
            "Bronze layer update complete\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2012,\n",
              " 2013,\n",
              " 2014,\n",
              " 2015,\n",
              " 2016,\n",
              " 2017,\n",
              " 2018,\n",
              " 2019,\n",
              " 2020,\n",
              " 2021,\n",
              " 2022,\n",
              " 2023,\n",
              " 2024,\n",
              " 2025]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def fetch_f1_bronze_layer_incremental(spark, bronze_path=\"/content/drive/MyDrive/Capstone/bronze/\",\n",
        "                                     checkpoint_path=\"/content/drive/MyDrive/Capstone/checkpoint/bronze\"):\n",
        "    \"\"\"\n",
        "    Build the bronze layer for F1 data with incremental loading support.\n",
        "    Stores data in race-per-row format with proper partitioning.\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    from pyspark.sql.functions import lit, current_timestamp\n",
        "    import json\n",
        "    import time\n",
        "    import requests\n",
        "\n",
        "    # Checkpoint logic remains the same\n",
        "    current_year = datetime.now().year  # Get once at start\n",
        "    try:\n",
        "        checkpoint_df = spark.read.parquet(checkpoint_path)\n",
        "        last_checkpoint = checkpoint_df.orderBy(\"timestamp\", ascending=False).first()\n",
        "        last_processed_season = last_checkpoint.last_processed_season\n",
        "        print(f\"Last processed season from checkpoint: {last_processed_season}\")\n",
        "        seasons_to_process = list(range(last_processed_season + 1, current_year + 1))\n",
        "    except:\n",
        "        seasons_to_process = list(range(1950, current_year + 1))\n",
        "        print(\"No checkpoint found - processing all seasons from 1950\")\n",
        "\n",
        "    # Force include current season (even if already processed)\n",
        "    if current_year not in seasons_to_process:\n",
        "        seasons_to_process.append(current_year)\n",
        "        seasons_to_process = sorted(seasons_to_process)\n",
        "        print(f\"Added current season {current_year} to processing list\")\n",
        "\n",
        "    print(f\"Processing seasons: {seasons_to_process}\")\n",
        "\n",
        "    # Rate limiter implementation remains the same\n",
        "    class RateLimiter:\n",
        "        def __init__(self, burst_limit=4, hourly_limit=500):\n",
        "            self.burst_limit = burst_limit\n",
        "            self.hourly_limit = hourly_limit\n",
        "            self.request_timestamps = []\n",
        "\n",
        "        def wait_if_needed(self):\n",
        "            current_time = time.time()\n",
        "            self.request_timestamps = [ts for ts in self.request_timestamps if current_time - ts < 3600]\n",
        "\n",
        "            if len(self.request_timestamps) >= self.hourly_limit:\n",
        "                oldest = min(self.request_timestamps)\n",
        "                sleep = 3600 - (current_time - oldest) + 1\n",
        "                print(f\"Hourly limit reached. Sleeping {sleep:.1f}s\")\n",
        "                time.sleep(sleep)\n",
        "                return self.wait_if_needed()\n",
        "\n",
        "            recent = [ts for ts in self.request_timestamps if current_time - ts < 1]\n",
        "            if len(recent) >= self.burst_limit:\n",
        "                time.sleep(1)\n",
        "\n",
        "            self.request_timestamps.append(current_time)\n",
        "\n",
        "    rate_limiter = RateLimiter()\n",
        "\n",
        "    def make_api_request(url, params, retries=3):\n",
        "        for attempt in range(retries):\n",
        "            rate_limiter.wait_if_needed()\n",
        "            try:\n",
        "                response = requests.get(url, params=params)\n",
        "                if response.status_code == 200:\n",
        "                    return response.json()\n",
        "                elif response.status_code == 429:\n",
        "                    wait = min(30, (2 ** attempt) + 1)\n",
        "                    print(f\"Rate limited. Waiting {wait}s\")\n",
        "                    time.sleep(wait)\n",
        "                else:\n",
        "                    print(f\"HTTP {response.status_code}. Retry {attempt+1}/{retries}\")\n",
        "                    time.sleep(1)\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {str(e)}. Retry {attempt+1}/{retries}\")\n",
        "                time.sleep(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_season_races(season):\n",
        "        \"\"\"Fetch and enrich races for a season\"\"\"\n",
        "        print(f\"Fetching season {season}\")\n",
        "        url = f\"https://api.jolpi.ca/ergast/f1/{season}/results/\"\n",
        "\n",
        "        # Get initial count\n",
        "        initial = make_api_request(url, {\"limit\": 1, \"offset\": 0})\n",
        "        if not initial or \"MRData\" not in initial:\n",
        "            return None\n",
        "\n",
        "        total = int(initial[\"MRData\"][\"total\"])\n",
        "        limit = 100\n",
        "        all_races = []\n",
        "\n",
        "        for offset in range(0, total, limit):\n",
        "            result = make_api_request(url, {\"limit\": limit, \"offset\": offset})\n",
        "            if result and \"MRData\" in result and \"RaceTable\" in result[\"MRData\"]:\n",
        "                all_races.extend(result[\"MRData\"][\"RaceTable\"].get(\"Races\", []))\n",
        "\n",
        "        # Add metadata to each race\n",
        "        enriched = []\n",
        "        ingestion_ts = datetime.now().isoformat()\n",
        "        for race in all_races:\n",
        "            race[\"season\"] = season\n",
        "            race[\"source\"] = \"jolpica_api\"\n",
        "            race[\"ingestion_timestamp\"] = ingestion_ts\n",
        "            enriched.append(race)\n",
        "\n",
        "        return enriched\n",
        "\n",
        "    # Process seasons\n",
        "    for season in seasons_to_process:\n",
        "        print(f\"\\nProcessing season {season}\")\n",
        "        season_dir = f\"{bronze_path}/season={season}\"\n",
        "\n",
        "        # Handle current season differently\n",
        "        if season == current_year:\n",
        "            print(f\"Force refreshing current season {season}\")\n",
        "\n",
        "            # Delete existing directory if it exists\n",
        "            try:\n",
        "                hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(season_dir)\n",
        "                fs = hadoop_path.getFileSystem(spark._jsc.hadoopConfiguration())\n",
        "                if fs.exists(hadoop_path):\n",
        "                    print(f\"Deleting existing data for season {season}\")\n",
        "                    fs.delete(hadoop_path, True)  # recursive delete\n",
        "            except Exception as e:\n",
        "                print(f\"Error deleting directory: {str(e)}\")\n",
        "                pass\n",
        "        else:\n",
        "            # Skip existing non-current seasons\n",
        "            try:\n",
        "                hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(season_dir)\n",
        "                fs = hadoop_path.getFileSystem(spark._jsc.hadoopConfiguration())\n",
        "                if fs.exists(hadoop_path):\n",
        "                    print(f\"Skipping existing non-current season {season}\")\n",
        "                    continue\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Rest of processing logic remains unchanged...\n",
        "        # Fetch data\n",
        "        races = fetch_season_races(season)\n",
        "        if not races:\n",
        "            print(f\"No data for season {season}\")\n",
        "            continue\n",
        "\n",
        "        # Write using Spark\n",
        "        sc = spark.sparkContext\n",
        "        rdd = sc.parallelize([json.dumps(race) for race in races])\n",
        "        rdd.coalesce(1).saveAsTextFile(season_dir)  # Now safe to write\n",
        "\n",
        "        # Update checkpoint\n",
        "        checkpoint_data = [{\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"last_processed_season\": season,\n",
        "            \"records_processed\": len(races)\n",
        "        }]\n",
        "        spark.createDataFrame(checkpoint_data).write.mode(\"append\").parquet(checkpoint_path)\n",
        "\n",
        "        print(f\"Saved {len(races)} races for season {season}\")\n",
        "\n",
        "    print(\"Bronze layer update complete\")\n",
        "    return seasons_to_process\n",
        "\n",
        "spark = SparkSession.builder.appName(\"F1MedallionPipeline\").getOrCreate()\n",
        "fetch_f1_bronze_layer_incremental(\n",
        "        spark)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnTdwNbyGnKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}