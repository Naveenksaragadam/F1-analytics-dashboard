{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mSgFUejQBc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT4MCVfKPyEO",
        "outputId": "7385da5e-95e5-41e4-f116-c5551bcb70a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/drive/MyDrive/Capstone/gold\n",
            "Created directory: /content/drive/MyDrive/Capstone/checkpoint/gold\n",
            "Created directory: /content/drive/MyDrive/Capstone/checkpoint/gold/dim_tables\n",
            "Created directory: /content/drive/MyDrive/Capstone/checkpoint/gold/fact_tables\n",
            "Created directory: /content/drive/MyDrive/Capstone/checkpoint/gold/aggregated_views\n",
            "Starting Gold Layer Processing...\n",
            "\\nProcessing dimension tables...\n",
            "Last processed timestamp: 1970-01-01T00:00:00\n",
            "Processing 25514 new records\n",
            "Initializing driver dimension\n",
            "Driver dimension written to /content/drive/MyDrive/Capstone/gold/dim_driver\n",
            "Initializing constructor dimension\n",
            "Constructor dimension written to /content/drive/MyDrive/Capstone/gold/dim_constructor\n",
            "Initializing circuit dimension\n",
            "Circuit dimension written to /content/drive/MyDrive/Capstone/gold/dim_circuit\n",
            "Checkpoint updated with timestamp: 2025-05-13 05:35:38.509089\n",
            "Dimension tables processing complete\n",
            "\\nProcessing fact tables...\n",
            "Last processed timestamp for facts: 1970-01-01T00:00:00\n",
            "Processing 25514 new records for fact tables\n",
            "Initializing race results fact table\n",
            "Race results fact written to /content/drive/MyDrive/Capstone/gold/fact_race_results\n",
            "Initializing driver standings fact table\n",
            "Driver standings fact written to /content/drive/MyDrive/Capstone/gold/fact_driver_standings\n",
            "Initializing constructor standings fact table\n",
            "Constructor standings fact written to /content/drive/MyDrive/Capstone/gold/fact_constructor_standings\n",
            "Checkpoint updated with timestamp: 2025-05-13 05:35:38.509089\n",
            "Fact tables processing complete\n",
            "\\nCreating aggregated views...\n",
            "Last processed timestamp for aggregated views: 1970-01-01T00:00:00\n",
            "Season summary view created\n",
            "Driver performance view created\n",
            "Constructor performance view created\n",
            "Circuit statistics view created\n",
            "Checkpoint updated with timestamp: 2025-05-13T06:12:59.291569\n",
            "Aggregated views creation complete\n",
            "\\nGold Layer Processing Complete!\n",
            "\\nSample of dimension data:\n",
            "+-----------------+-----------------+------------------+-----------------+--------------------+--------------------+------------+----------+\n",
            "|        driver_id|driver_given_name|driver_family_name| driver_full_name| processed_timestamp|      effective_from|effective_to|is_current|\n",
            "+-----------------+-----------------+------------------+-----------------+--------------------+--------------------+------------+----------+\n",
            "|           ascari|          Alberto|            Ascari|   Alberto Ascari|2025-05-13 05:35:...|2025-05-13 06:11:...|        NULL|      true|\n",
            "|            musso|            Luigi|             Musso|      Luigi Musso|2025-05-13 05:35:...|2025-05-13 06:11:...|        NULL|      true|\n",
            "|            creus|          Antonio|             Creus|    Antonio Creus|2025-05-13 05:35:...|2025-05-13 06:11:...|        NULL|      true|\n",
            "|wilson_fittipaldi|           Wilson|        Fittipaldi|Wilson Fittipaldi|2025-05-13 05:35:...|2025-05-13 06:11:...|        NULL|      true|\n",
            "|             lees|            Geoff|              Lees|       Geoff Lees|2025-05-13 05:35:...|2025-05-13 06:11:...|        NULL|      true|\n",
            "+-----------------+-----------------+------------------+-----------------+--------------------+--------------------+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\\nSample of fact data:\n",
            "+--------------------+-----+-------------------+----------+--------------+--------------+--------+------+----+----+--------+-----------+--------------------+------+\n",
            "|           result_id|round|     race_timestamp|circuit_id|     driver_id|constructor_id|position|points|grid|laps|  status|finish_time| processed_timestamp|season|\n",
            "+--------------------+-----+-------------------+----------+--------------+--------------+--------+------+----+----+--------+-----------+--------------------+------+\n",
            "|2024-1-max_versta...|    1|2024-03-02 15:00:00|   bahrain|max_verstappen|      red_bull|       1|  26.0|   1|  57|Finished|1:31:44.742|2025-05-13 05:35:...|  2024|\n",
            "|        2024-1-perez|    1|2024-03-02 15:00:00|   bahrain|         perez|      red_bull|       2|  18.0|   5|  57|Finished|    +22.457|2025-05-13 05:35:...|  2024|\n",
            "|        2024-1-sainz|    1|2024-03-02 15:00:00|   bahrain|         sainz|       ferrari|       3|  15.0|   4|  57|Finished|    +25.110|2025-05-13 05:35:...|  2024|\n",
            "|      2024-1-leclerc|    1|2024-03-02 15:00:00|   bahrain|       leclerc|       ferrari|       4|  12.0|   2|  57|Finished|    +39.669|2025-05-13 05:35:...|  2024|\n",
            "|      2024-1-russell|    1|2024-03-02 15:00:00|   bahrain|       russell|      mercedes|       5|  10.0|   3|  57|Finished|    +46.788|2025-05-13 05:35:...|  2024|\n",
            "+--------------------+-----+-------------------+----------+--------------+--------------+--------+------+----+----+--------+-----------+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\\nSample of aggregated view:\n",
            "+------+-----------+-------------+------------------+---------------+------------------+---------------+-------------+----------------+--------------------+\n",
            "|season|total_races|total_drivers|total_constructors|champion_driver|champion_driver_id|champion_points|champion_team|champion_team_id|champion_team_points|\n",
            "+------+-----------+-------------+------------------+---------------+------------------+---------------+-------------+----------------+--------------------+\n",
            "|  2025|          6|           20|                10|  Oscar Piastri|           piastri|          117.0|      McLaren|         mclaren|               223.0|\n",
            "|  2024|         24|           24|                10| Max Verstappen|    max_verstappen|          399.0|      McLaren|         mclaren|               609.0|\n",
            "|  2023|         22|           22|                10| Max Verstappen|    max_verstappen|          530.0|     Red Bull|        red_bull|               790.0|\n",
            "|  2022|         22|           22|                10| Max Verstappen|    max_verstappen|          433.0|     Red Bull|        red_bull|               724.0|\n",
            "|  2021|         22|           21|                10| Max Verstappen|    max_verstappen|          388.5|     Mercedes|        mercedes|               604.5|\n",
            "+------+-----------+-------------+------------------+---------------+------------------+---------------+-------------+----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a gold_layer.py file with the same functionality\n",
        "\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Initialize Spark Session\n",
        "def initialize_spark():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"F1GoldLayer\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "def get_last_processed_timestamp(spark, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Get the last processed timestamp from checkpoint\n",
        "    \"\"\"\n",
        "    try:\n",
        "        checkpoint_df = spark.read.parquet(checkpoint_path)\n",
        "        last_checkpoint = checkpoint_df.orderBy(\"processing_timestamp\", ascending=False).first()\n",
        "        return last_checkpoint.last_processed_timestamp\n",
        "    except:\n",
        "        # Return a default timestamp if no checkpoint exists\n",
        "        return datetime(1970, 1, 1).isoformat()\n",
        "\n",
        "def update_checkpoint(spark, checkpoint_path, last_processed_timestamp):\n",
        "    \"\"\"\n",
        "    Update the checkpoint with the latest processing information\n",
        "    \"\"\"\n",
        "    checkpoint_data = [{\n",
        "        \"processing_timestamp\": datetime.now().isoformat(),\n",
        "        \"last_processed_timestamp\": last_processed_timestamp,\n",
        "    }]\n",
        "\n",
        "    checkpoint_df = spark.createDataFrame(checkpoint_data)\n",
        "    checkpoint_df.write.mode(\"append\").parquet(checkpoint_path)\n",
        "    print(f\"Checkpoint updated with timestamp: {last_processed_timestamp}\")\n",
        "\n",
        "def apply_scd_type2(spark, current_df, new_df, key_columns, track_columns,\n",
        "                   effective_from_col=\"effective_from\",\n",
        "                   effective_to_col=\"effective_to\",\n",
        "                   current_flag_col=\"is_current\"):\n",
        "    \"\"\"\n",
        "    Apply SCD Type 2 logic to merge current and new data\n",
        "\n",
        "    Parameters:\n",
        "    - current_df: Current dimension table\n",
        "    - new_df: New data to be merged\n",
        "    - key_columns: List of columns that uniquely identify a record\n",
        "    - track_columns: List of columns to track changes\n",
        "    - effective_from_col: Column name for effective from date\n",
        "    - effective_to_col: Column name for effective to date\n",
        "    - current_flag_col: Column name for current flag\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with SCD Type 2 applied\n",
        "    \"\"\"\n",
        "    # If current_df is empty, initialize with new_df\n",
        "    if current_df.rdd.isEmpty():\n",
        "        return new_df.withColumn(effective_from_col, F.current_timestamp()) \\\n",
        "                    .withColumn(effective_to_col, F.lit(None).cast(\"timestamp\")) \\\n",
        "                    .withColumn(current_flag_col, F.lit(True))\n",
        "\n",
        "    # Register DataFrames as temp views for SQL operations\n",
        "    current_df.createOrReplaceTempView(\"current_df\")\n",
        "    new_df.createOrReplaceTempView(\"new_df\")\n",
        "\n",
        "    # Identify key columns in both dataframes\n",
        "    join_condition = \" AND \".join([f\"current.{col} = new.{col}\" for col in key_columns])\n",
        "\n",
        "    # Identify changed records\n",
        "    change_condition = \" OR \".join([f\"(current.{col} <> new.{col} OR (current.{col} IS NULL AND new.{col} IS NOT NULL) OR (current.{col} IS NOT NULL AND new.{col} IS NULL))\" for col in track_columns])\n",
        "\n",
        "    # 1. Expire current records that have changes\n",
        "    expired_records = spark.sql(f\"\"\"\n",
        "        SELECT\n",
        "            current.*,\n",
        "            FALSE as {current_flag_col},\n",
        "            current_timestamp() as {effective_to_col}\n",
        "        FROM current_df current\n",
        "        JOIN new_df new\n",
        "        ON {join_condition}\n",
        "        WHERE current.{current_flag_col} = TRUE\n",
        "        AND ({change_condition})\n",
        "    \"\"\")\n",
        "\n",
        "    # 2. Insert new versions of changed records\n",
        "    new_versions = spark.sql(f\"\"\"\n",
        "        SELECT\n",
        "            new.*,\n",
        "            current_timestamp() as {effective_from_col},\n",
        "            NULL as {effective_to_col},\n",
        "            TRUE as {current_flag_col}\n",
        "        FROM new_df new\n",
        "        JOIN current_df current\n",
        "        ON {join_condition}\n",
        "        WHERE current.{current_flag_col} = TRUE\n",
        "        AND ({change_condition})\n",
        "    \"\"\")\n",
        "\n",
        "    # 3. Insert completely new records\n",
        "    new_records = spark.sql(f\"\"\"\n",
        "        SELECT\n",
        "            new.*,\n",
        "            current_timestamp() as {effective_from_col},\n",
        "            NULL as {effective_to_col},\n",
        "            TRUE as {current_flag_col}\n",
        "        FROM new_df new\n",
        "        LEFT JOIN current_df current\n",
        "        ON {join_condition}\n",
        "        WHERE current.{key_columns[0]} IS NULL\n",
        "    \"\"\")\n",
        "\n",
        "    # 4. Keep unchanged current records\n",
        "    unchanged_records = spark.sql(f\"\"\"\n",
        "        SELECT current.*\n",
        "        FROM current_df current\n",
        "        JOIN new_df new\n",
        "        ON {join_condition}\n",
        "        WHERE current.{current_flag_col} = TRUE\n",
        "        AND NOT ({change_condition})\n",
        "    \"\"\")\n",
        "\n",
        "    # 5. Keep historical records\n",
        "    historical_records = spark.sql(f\"\"\"\n",
        "        SELECT current.*\n",
        "        FROM current_df current\n",
        "        WHERE current.{current_flag_col} = FALSE\n",
        "    \"\"\")\n",
        "\n",
        "    # Union all results\n",
        "    result_df = expired_records.unionAll(new_versions) \\\n",
        "                              .unionAll(new_records) \\\n",
        "                              .unionAll(unchanged_records) \\\n",
        "                              .unionAll(historical_records)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def process_driver_dimension(spark, silver_df, gold_path):\n",
        "    \"\"\"Process driver dimension with SCD Type 2\"\"\"\n",
        "    # Extract driver data from silver\n",
        "    driver_df = spark.sql(\"\"\"\n",
        "        SELECT DISTINCT\n",
        "            driver_id,\n",
        "            driver_given_name,\n",
        "            driver_family_name,\n",
        "            driver_full_name,\n",
        "            processed_timestamp\n",
        "        FROM silver_data\n",
        "    \"\"\")\n",
        "\n",
        "    # Define path for driver dimension\n",
        "    driver_dim_path = f\"{gold_path}/dim_driver\"\n",
        "\n",
        "    # Check if dimension exists\n",
        "    try:\n",
        "        current_driver_dim = spark.read.parquet(driver_dim_path)\n",
        "        current_driver_dim.createOrReplaceTempView(\"current_df\")\n",
        "        driver_df.createOrReplaceTempView(\"new_df\")\n",
        "\n",
        "        # Apply SCD Type 2\n",
        "        updated_driver_dim = apply_scd_type2(\n",
        "            spark,\n",
        "            current_driver_dim,\n",
        "            driver_df,\n",
        "            key_columns=[\"driver_id\"],\n",
        "            track_columns=[\"driver_given_name\", \"driver_family_name\", \"driver_full_name\"]\n",
        "        )\n",
        "    except:\n",
        "        # Initialize dimension if it doesn't exist\n",
        "        print(\"Initializing driver dimension\")\n",
        "        updated_driver_dim = driver_df.withColumn(\"effective_from\", F.current_timestamp()) \\\n",
        "                                    .withColumn(\"effective_to\", F.lit(None).cast(\"timestamp\")) \\\n",
        "                                    .withColumn(\"is_current\", F.lit(True))\n",
        "\n",
        "    # Write dimension table\n",
        "    updated_driver_dim.write.mode(\"overwrite\").parquet(driver_dim_path)\n",
        "    print(f\"Driver dimension written to {driver_dim_path}\")\n",
        "\n",
        "def process_constructor_dimension(spark, silver_df, gold_path):\n",
        "    \"\"\"Process constructor dimension with SCD Type 2\"\"\"\n",
        "    # Extract constructor data from silver\n",
        "    constructor_df = spark.sql(\"\"\"\n",
        "        SELECT DISTINCT\n",
        "            constructor_id,\n",
        "            constructor_name,\n",
        "            processed_timestamp\n",
        "        FROM silver_data\n",
        "    \"\"\")\n",
        "\n",
        "    # Define path for constructor dimension\n",
        "    constructor_dim_path = f\"{gold_path}/dim_constructor\"\n",
        "\n",
        "    # Check if dimension exists\n",
        "    try:\n",
        "        current_constructor_dim = spark.read.parquet(constructor_dim_path)\n",
        "        current_constructor_dim.createOrReplaceTempView(\"current_df\")\n",
        "        constructor_df.createOrReplaceTempView(\"new_df\")\n",
        "\n",
        "        # Apply SCD Type 2\n",
        "        updated_constructor_dim = apply_scd_type2(\n",
        "            spark,\n",
        "            current_constructor_dim,\n",
        "            constructor_df,\n",
        "            key_columns=[\"constructor_id\"],\n",
        "            track_columns=[\"constructor_name\"]\n",
        "        )\n",
        "    except:\n",
        "        # Initialize dimension if it doesn't exist\n",
        "        print(\"Initializing constructor dimension\")\n",
        "        updated_constructor_dim = constructor_df.withColumn(\"effective_from\", F.current_timestamp()) \\\n",
        "                                             .withColumn(\"effective_to\", F.lit(None).cast(\"timestamp\")) \\\n",
        "                                             .withColumn(\"is_current\", F.lit(True))\n",
        "\n",
        "    # Write dimension table\n",
        "    updated_constructor_dim.write.mode(\"overwrite\").parquet(constructor_dim_path)\n",
        "    print(f\"Constructor dimension written to {constructor_dim_path}\")\n",
        "\n",
        "def process_circuit_dimension(spark, silver_df, gold_path):\n",
        "    \"\"\"Process circuit dimension with SCD Type 2\"\"\"\n",
        "    # Extract circuit data from silver\n",
        "    circuit_df = spark.sql(\"\"\"\n",
        "        SELECT DISTINCT\n",
        "            circuit_id,\n",
        "            circuit_name,\n",
        "            circuit_lat,\n",
        "            circuit_long,\n",
        "            circuit_locality,\n",
        "            circuit_country,\n",
        "            processed_timestamp\n",
        "        FROM silver_data\n",
        "    \"\"\")\n",
        "\n",
        "    # Define path for circuit dimension\n",
        "    circuit_dim_path = f\"{gold_path}/dim_circuit\"\n",
        "\n",
        "    # Check if dimension exists\n",
        "    try:\n",
        "        current_circuit_dim = spark.read.parquet(circuit_dim_path)\n",
        "        current_circuit_dim.createOrReplaceTempView(\"current_df\")\n",
        "        circuit_df.createOrReplaceTempView(\"new_df\")\n",
        "\n",
        "        # Apply SCD Type 2\n",
        "        updated_circuit_dim = apply_scd_type2(\n",
        "            spark,\n",
        "            current_circuit_dim,\n",
        "            circuit_df,\n",
        "            key_columns=[\"circuit_id\"],\n",
        "            track_columns=[\"circuit_name\", \"circuit_lat\", \"circuit_long\", \"circuit_locality\", \"circuit_country\"]\n",
        "        )\n",
        "    except:\n",
        "        # Initialize dimension if it doesn't exist\n",
        "        print(\"Initializing circuit dimension\")\n",
        "        updated_circuit_dim = circuit_df.withColumn(\"effective_from\", F.current_timestamp()) \\\n",
        "                                      .withColumn(\"effective_to\", F.lit(None).cast(\"timestamp\")) \\\n",
        "                                      .withColumn(\"is_current\", F.lit(True))\n",
        "\n",
        "    # Write dimension table\n",
        "    updated_circuit_dim.write.mode(\"overwrite\").parquet(circuit_dim_path)\n",
        "    print(f\"Circuit dimension written to {circuit_dim_path}\")\n",
        "\n",
        "def process_dimension_tables(spark, silver_path, gold_path, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Process dimension tables with SCD Type 2\n",
        "    \"\"\"\n",
        "    # Get the last processed timestamp\n",
        "    last_processed_ts = get_last_processed_timestamp(spark, f\"{checkpoint_path}/dim_tables\")\n",
        "    print(f\"Last processed timestamp: {last_processed_ts}\")\n",
        "\n",
        "    # Read silver data with incremental filter\n",
        "    silver_df = spark.read.parquet(silver_path) \\\n",
        "                    .filter(F.col(\"processed_timestamp\") > F.lit(last_processed_ts))\n",
        "\n",
        "    if silver_df.rdd.isEmpty():\n",
        "        print(\"No new data to process\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {silver_df.count()} new records\")\n",
        "\n",
        "    # Get current max timestamp for checkpoint update\n",
        "    max_timestamp = silver_df.agg(F.max(\"processed_timestamp\")).collect()[0][0]\n",
        "\n",
        "    # Register silver_df as a temp view for SQL operations\n",
        "    silver_df.createOrReplaceTempView(\"silver_data\")\n",
        "\n",
        "    # Process Driver dimension\n",
        "    process_driver_dimension(spark, silver_df, gold_path)\n",
        "\n",
        "    # Process Constructor dimension\n",
        "    process_constructor_dimension(spark, silver_df, gold_path)\n",
        "\n",
        "    # Process Circuit dimension\n",
        "    process_circuit_dimension(spark, silver_df, gold_path)\n",
        "\n",
        "    # Update checkpoint\n",
        "    update_checkpoint(spark, f\"{checkpoint_path}/dim_tables\", max_timestamp)\n",
        "\n",
        "    print(\"Dimension tables processing complete\")\n",
        "\n",
        "def process_race_results_fact(spark, silver_df, gold_path):\n",
        "    \"\"\"Process race results fact table incrementally\"\"\"\n",
        "    # Extract race results data\n",
        "    race_results_df = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            CONCAT(season, '-', round, '-', driver_id) as result_id,\n",
        "            season,\n",
        "            round,\n",
        "            race_timestamp,\n",
        "            circuit_id,\n",
        "            driver_id,\n",
        "            constructor_id,\n",
        "            CAST(position as INT) as position,\n",
        "            CAST(points as DOUBLE) as points,\n",
        "            CAST(grid as INT) as grid,\n",
        "            CAST(laps as INT) as laps,\n",
        "            status,\n",
        "            finish_time,\n",
        "            processed_timestamp\n",
        "        FROM silver_data\n",
        "        WHERE is_valid_record = TRUE\n",
        "    \"\"\")\n",
        "\n",
        "    # Define path for race results fact\n",
        "    race_results_path = f\"{gold_path}/fact_race_results\"\n",
        "\n",
        "    # Check if fact table exists and append new data\n",
        "    try:\n",
        "        # Read existing data\n",
        "        existing_results = spark.read.parquet(race_results_path)\n",
        "\n",
        "        # Find records that already exist (to avoid duplicates)\n",
        "        joined_df = race_results_df.join(\n",
        "            existing_results,\n",
        "            on=[\"result_id\"],\n",
        "            how=\"left_anti\"  # Only keep records that don't exist in the target\n",
        "        )\n",
        "\n",
        "        # Append new records\n",
        "        if not joined_df.rdd.isEmpty():\n",
        "            joined_df.write.mode(\"append\").partitionBy(\"season\").parquet(race_results_path)\n",
        "            print(f\"Added {joined_df.count()} new records to race results fact\")\n",
        "        else:\n",
        "            print(\"No new race results to add\")\n",
        "\n",
        "    except:\n",
        "        # Initialize fact table if it doesn't exist\n",
        "        print(\"Initializing race results fact table\")\n",
        "        race_results_df.write.partitionBy(\"season\").parquet(race_results_path)\n",
        "        print(f\"Race results fact written to {race_results_path}\")\n",
        "\n",
        "def process_driver_standings_fact(spark, silver_df, gold_path):\n",
        "    \"\"\"Process driver standings fact table incrementally\"\"\"\n",
        "    # Calculate driver standings\n",
        "    driver_standings_df = spark.sql(\"\"\"\n",
        "        WITH race_points AS (\n",
        "            SELECT\n",
        "                season,\n",
        "                round,\n",
        "                driver_id,\n",
        "                CAST(points as DOUBLE) as race_points\n",
        "            FROM silver_data\n",
        "            WHERE is_valid_record = TRUE\n",
        "        ),\n",
        "        cumulative_points AS (\n",
        "            SELECT\n",
        "                season,\n",
        "                round,\n",
        "                driver_id,\n",
        "                race_points,\n",
        "                SUM(race_points) OVER (\n",
        "                    PARTITION BY season, driver_id\n",
        "                    ORDER BY round\n",
        "                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "                ) as total_points,\n",
        "                ROW_NUMBER() OVER (\n",
        "                    PARTITION BY season, round\n",
        "                    ORDER BY SUM(race_points) OVER (\n",
        "                        PARTITION BY season, driver_id\n",
        "                        ORDER BY round\n",
        "                        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "                    ) DESC\n",
        "                ) as position\n",
        "            FROM race_points\n",
        "        )\n",
        "        SELECT\n",
        "            CONCAT(season, '-', round, '-', driver_id) as standing_id,\n",
        "            season,\n",
        "            round,\n",
        "            driver_id,\n",
        "            race_points,\n",
        "            total_points,\n",
        "            position,\n",
        "            current_timestamp() as processed_timestamp\n",
        "        FROM cumulative_points\n",
        "    \"\"\")\n",
        "\n",
        "    # Define path for driver standings fact\n",
        "    driver_standings_path = f\"{gold_path}/fact_driver_standings\"\n",
        "\n",
        "    # Check if fact table exists and append new data\n",
        "    try:\n",
        "        # Read existing data\n",
        "        existing_standings = spark.read.parquet(driver_standings_path)\n",
        "\n",
        "        # Find records that already exist (to avoid duplicates)\n",
        "        joined_df = driver_standings_df.join(\n",
        "            existing_standings,\n",
        "            on=[\"standing_id\"],\n",
        "            how=\"left_anti\"  # Only keep records that don't exist in the target\n",
        "        )\n",
        "\n",
        "        # Append new records\n",
        "        if not joined_df.rdd.isEmpty():\n",
        "            joined_df.write.mode(\"append\").partitionBy(\"season\").parquet(driver_standings_path)\n",
        "            print(f\"Added {joined_df.count()} new records to driver standings fact\")\n",
        "        else:\n",
        "            print(\"No new driver standings to add\")\n",
        "\n",
        "    except:\n",
        "        # Initialize fact table if it doesn't exist\n",
        "        print(\"Initializing driver standings fact table\")\n",
        "        driver_standings_df.write.partitionBy(\"season\").parquet(driver_standings_path)\n",
        "        print(f\"Driver standings fact written to {driver_standings_path}\")\n",
        "\n",
        "def process_constructor_standings_fact(spark, silver_df, gold_path):\n",
        "    \"\"\"Process constructor standings fact table incrementally\"\"\"\n",
        "    # Calculate constructor standings\n",
        "    constructor_standings_df = spark.sql(\"\"\"\n",
        "        WITH constructor_points AS (\n",
        "            SELECT\n",
        "                season,\n",
        "                round,\n",
        "                constructor_id,\n",
        "                SUM(CAST(points as DOUBLE)) as race_points\n",
        "            FROM silver_data\n",
        "            WHERE is_valid_record = TRUE\n",
        "            GROUP BY season, round, constructor_id\n",
        "        ),\n",
        "        cumulative_points AS (\n",
        "            SELECT\n",
        "                season,\n",
        "                round,\n",
        "                constructor_id,\n",
        "                race_points,\n",
        "                SUM(race_points) OVER (\n",
        "                    PARTITION BY season, constructor_id\n",
        "                    ORDER BY round\n",
        "                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "                ) as total_points,\n",
        "                ROW_NUMBER() OVER (\n",
        "                    PARTITION BY season, round\n",
        "                    ORDER BY SUM(race_points) OVER (\n",
        "                        PARTITION BY season, constructor_id\n",
        "                        ORDER BY round\n",
        "                        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "                    ) DESC\n",
        "                ) as position\n",
        "            FROM constructor_points\n",
        "        )\n",
        "        SELECT\n",
        "            CONCAT(season, '-', round, '-', constructor_id) as standing_id,\n",
        "            season,\n",
        "            round,\n",
        "            constructor_id,\n",
        "            race_points,\n",
        "            total_points,\n",
        "            position,\n",
        "            current_timestamp() as processed_timestamp\n",
        "        FROM cumulative_points\n",
        "    \"\"\")\n",
        "\n",
        "    # Define path for constructor standings fact\n",
        "    constructor_standings_path = f\"{gold_path}/fact_constructor_standings\"\n",
        "\n",
        "    # Check if fact table exists and append new data\n",
        "    try:\n",
        "        # Read existing data\n",
        "        existing_standings = spark.read.parquet(constructor_standings_path)\n",
        "\n",
        "        # Find records that already exist (to avoid duplicates)\n",
        "        joined_df = constructor_standings_df.join(\n",
        "            existing_standings,\n",
        "            on=[\"standing_id\"],\n",
        "            how=\"left_anti\"  # Only keep records that don't exist in the target\n",
        "        )\n",
        "\n",
        "        # Append new records\n",
        "        if not joined_df.rdd.isEmpty():\n",
        "            joined_df.write.mode(\"append\").partitionBy(\"season\").parquet(constructor_standings_path)\n",
        "            print(f\"Added {joined_df.count()} new records to constructor standings fact\")\n",
        "        else:\n",
        "            print(\"No new constructor standings to add\")\n",
        "\n",
        "    except:\n",
        "        # Initialize fact table if it doesn't exist\n",
        "        print(\"Initializing constructor standings fact table\")\n",
        "        constructor_standings_df.write.partitionBy(\"season\").parquet(constructor_standings_path)\n",
        "        print(f\"Constructor standings fact written to {constructor_standings_path}\")\n",
        "\n",
        "def process_fact_tables(spark, silver_path, gold_path, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Process fact tables incrementally\n",
        "    \"\"\"\n",
        "    # Get the last processed timestamp\n",
        "    last_processed_ts = get_last_processed_timestamp(spark, f\"{checkpoint_path}/fact_tables\")\n",
        "    print(f\"Last processed timestamp for facts: {last_processed_ts}\")\n",
        "\n",
        "    # Read silver data with incremental filter\n",
        "    silver_df = spark.read.parquet(silver_path) \\\n",
        "                    .filter(F.col(\"processed_timestamp\") > F.lit(last_processed_ts))\n",
        "\n",
        "    if silver_df.rdd.isEmpty():\n",
        "        print(\"No new data to process for fact tables\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {silver_df.count()} new records for fact tables\")\n",
        "\n",
        "    # Get current max timestamp for checkpoint update\n",
        "    max_timestamp = silver_df.agg(F.max(\"processed_timestamp\")).collect()[0][0]\n",
        "\n",
        "    # Register silver_df as a temp view for SQL operations\n",
        "    silver_df.createOrReplaceTempView(\"silver_data\")\n",
        "\n",
        "    # Process Race Results fact table\n",
        "    process_race_results_fact(spark, silver_df, gold_path)\n",
        "\n",
        "    # Process Driver Standings fact table\n",
        "    process_driver_standings_fact(spark, silver_df, gold_path)\n",
        "\n",
        "    # Process Constructor Standings fact table\n",
        "    process_constructor_standings_fact(spark, silver_df, gold_path)\n",
        "\n",
        "    # Update checkpoint\n",
        "    update_checkpoint(spark, f\"{checkpoint_path}/fact_tables\", max_timestamp)\n",
        "\n",
        "    print(\"Fact tables processing complete\")\n",
        "\n",
        "def create_season_summary_view(spark, gold_path):\n",
        "    \"\"\"Create season summary view\"\"\"\n",
        "    season_summary_df = spark.sql(\"\"\"\n",
        "        WITH season_winners AS (\n",
        "            SELECT\n",
        "                fds.season,\n",
        "                dd.driver_full_name as champion_driver,\n",
        "                fds.driver_id as champion_driver_id,\n",
        "                fds.total_points as champion_points\n",
        "            FROM fact_driver_standings fds\n",
        "            JOIN dim_driver dd ON fds.driver_id = dd.driver_id\n",
        "            WHERE fds.round = (\n",
        "                SELECT MAX(round)\n",
        "                FROM fact_driver_standings\n",
        "                WHERE season = fds.season\n",
        "            )\n",
        "            AND fds.position = 1\n",
        "        ),\n",
        "        constructor_winners AS (\n",
        "            SELECT\n",
        "                fcs.season,\n",
        "                dc.constructor_name as champion_team,\n",
        "                fcs.constructor_id as champion_team_id,\n",
        "                fcs.total_points as champion_team_points\n",
        "            FROM fact_constructor_standings fcs\n",
        "            JOIN dim_constructor dc ON fcs.constructor_id = dc.constructor_id\n",
        "            WHERE fcs.round = (\n",
        "                SELECT MAX(round)\n",
        "                FROM fact_constructor_standings\n",
        "                WHERE season = fcs.season\n",
        "            )\n",
        "            AND fcs.position = 1\n",
        "        ),\n",
        "        season_stats AS (\n",
        "            SELECT\n",
        "                season,\n",
        "                COUNT(DISTINCT round) as total_races,\n",
        "                COUNT(DISTINCT driver_id) as total_drivers,\n",
        "                COUNT(DISTINCT constructor_id) as total_constructors\n",
        "            FROM fact_race_results\n",
        "            GROUP BY season\n",
        "        )\n",
        "        SELECT\n",
        "            ss.season,\n",
        "            ss.total_races,\n",
        "            ss.total_drivers,\n",
        "            ss.total_constructors,\n",
        "            sw.champion_driver,\n",
        "            sw.champion_driver_id,\n",
        "            sw.champion_points,\n",
        "            cw.champion_team,\n",
        "            cw.champion_team_id,\n",
        "            cw.champion_team_points\n",
        "        FROM season_stats ss\n",
        "        LEFT JOIN season_winners sw ON ss.season = sw.season\n",
        "        LEFT JOIN constructor_winners cw ON ss.season = cw.season\n",
        "        ORDER BY ss.season DESC\n",
        "    \"\"\")\n",
        "\n",
        "    # Write the view\n",
        "    season_summary_df.write.mode(\"overwrite\").parquet(f\"{gold_path}/view_season_summary\")\n",
        "    print(\"Season summary view created\")\n",
        "\n",
        "def create_driver_performance_view(spark, gold_path):\n",
        "    \"\"\"Create driver performance view\"\"\"\n",
        "    driver_performance_df = spark.sql(\"\"\"\n",
        "        WITH driver_stats AS (\n",
        "            SELECT\n",
        "                frr.season,\n",
        "                frr.driver_id,\n",
        "                dd.driver_full_name,\n",
        "                COUNT(DISTINCT CONCAT(frr.season, '-', frr.round)) as races,\n",
        "                SUM(CASE WHEN frr.position = 1 THEN 1 ELSE 0 END) as wins,\n",
        "                SUM(CASE WHEN frr.position = 2 THEN 1 ELSE 0 END) as p2,\n",
        "                SUM(CASE WHEN frr.position = 3 THEN 1 ELSE 0 END) as p3,\n",
        "                SUM(CASE WHEN frr.position BETWEEN 1 AND 3 THEN 1 ELSE 0 END) as podiums,\n",
        "                SUM(CASE WHEN frr.position BETWEEN 1 AND 10 THEN 1 ELSE 0 END) as points_finishes,\n",
        "                SUM(frr.points) as total_points,\n",
        "                AVG(CAST(frr.position as DOUBLE)) as avg_position,\n",
        "                MIN(CAST(frr.position as INT)) as best_position,\n",
        "                AVG(CAST(frr.grid as DOUBLE)) as avg_grid,\n",
        "                SUM(CASE WHEN frr.grid = 1 THEN 1 ELSE 0 END) as poles\n",
        "            FROM fact_race_results frr\n",
        "            JOIN dim_driver dd ON frr.driver_id = dd.driver_id\n",
        "            GROUP BY frr.season, frr.driver_id, dd.driver_full_name\n",
        "        )\n",
        "        SELECT\n",
        "            ds.*,\n",
        "            ROUND(ds.wins / NULLIF(ds.races, 0) * 100, 2) as win_percentage,\n",
        "            ROUND(ds.podiums / NULLIF(ds.races, 0) * 100, 2) as podium_percentage,\n",
        "            ROUND(ds.total_points / NULLIF(ds.races, 0), 2) as points_per_race\n",
        "        FROM driver_stats ds\n",
        "        ORDER BY ds.season DESC, ds.total_points DESC\n",
        "    \"\"\")\n",
        "\n",
        "    # Write the view\n",
        "    driver_performance_df.write.mode(\"overwrite\").partitionBy(\"season\").parquet(f\"{gold_path}/view_driver_performance\")\n",
        "    print(\"Driver performance view created\")\n",
        "\n",
        "def create_constructor_performance_view(spark, gold_path):\n",
        "    \"\"\"Create constructor performance view\"\"\"\n",
        "    constructor_performance_df = spark.sql(\"\"\"\n",
        "        WITH constructor_stats AS (\n",
        "            SELECT\n",
        "                frr.season,\n",
        "                frr.constructor_id,\n",
        "                dc.constructor_name,\n",
        "                COUNT(DISTINCT CONCAT(frr.season, '-', frr.round)) as races,\n",
        "                SUM(CASE WHEN frr.position = 1 THEN 1 ELSE 0 END) as wins,\n",
        "                SUM(CASE WHEN frr.position BETWEEN 1 AND 3 THEN 1 ELSE 0 END) as podiums,\n",
        "                SUM(CASE WHEN frr.position BETWEEN 1 AND 10 THEN 1 ELSE 0 END) as points_finishes,\n",
        "                SUM(frr.points) as total_points,\n",
        "                COUNT(DISTINCT frr.driver_id) as drivers_used\n",
        "            FROM fact_race_results frr\n",
        "            JOIN dim_constructor dc ON frr.constructor_id = dc.constructor_id\n",
        "            GROUP BY frr.season, frr.constructor_id, dc.constructor_name\n",
        "        )\n",
        "        SELECT\n",
        "            cs.*,\n",
        "            ROUND(cs.wins / NULLIF(cs.races, 0) * 100, 2) as win_percentage,\n",
        "            ROUND(cs.podiums / NULLIF(cs.races, 0) * 100, 2) as podium_percentage,\n",
        "            ROUND(cs.total_points / NULLIF(cs.races, 0), 2) as points_per_race\n",
        "        FROM constructor_stats cs\n",
        "        ORDER BY cs.season DESC, cs.total_points DESC\n",
        "    \"\"\")\n",
        "\n",
        "    # Write the view\n",
        "    constructor_performance_df.write.mode(\"overwrite\").partitionBy(\"season\").parquet(f\"{gold_path}/view_constructor_performance\")\n",
        "    print(\"Constructor performance view created\")\n",
        "\n",
        "def create_circuit_statistics_view(spark, gold_path):\n",
        "    \"\"\"Create circuit statistics view\"\"\"\n",
        "    circuit_statistics_df = spark.sql(\"\"\"\n",
        "        WITH circuit_stats AS (\n",
        "            SELECT\n",
        "                frr.circuit_id,\n",
        "                dc.circuit_name,\n",
        "                dc.circuit_country,\n",
        "                COUNT(DISTINCT frr.season) as seasons_used,\n",
        "                COUNT(DISTINCT CONCAT(frr.season, '-', frr.round)) as total_races,\n",
        "                AVG(CAST(frr.laps as DOUBLE)) as avg_race_laps,\n",
        "                COUNT(DISTINCT frr.driver_id) as different_winners,\n",
        "                COUNT(DISTINCT frr.constructor_id) as different_winning_constructors\n",
        "            FROM fact_race_results frr\n",
        "            JOIN dim_circuit dc ON frr.circuit_id = dc.circuit_id\n",
        "            WHERE frr.position = 1\n",
        "            GROUP BY frr.circuit_id, dc.circuit_name, dc.circuit_country\n",
        "        ),\n",
        "        most_successful_drivers AS (\n",
        "            SELECT\n",
        "                frr.circuit_id,\n",
        "                dd.driver_full_name,\n",
        "                COUNT(*) as wins,\n",
        "                ROW_NUMBER() OVER (PARTITION BY frr.circuit_id ORDER BY COUNT(*) DESC) as rank\n",
        "            FROM fact_race_results frr\n",
        "            JOIN dim_driver dd ON frr.driver_id = dd.driver_id\n",
        "            WHERE frr.position = 1\n",
        "            GROUP BY frr.circuit_id, dd.driver_full_name\n",
        "        ),\n",
        "        most_successful_constructors AS (\n",
        "            SELECT\n",
        "                frr.circuit_id,\n",
        "                dc.constructor_name,\n",
        "                COUNT(*) as wins,\n",
        "                ROW_NUMBER() OVER (PARTITION BY frr.circuit_id ORDER BY COUNT(*) DESC) as rank\n",
        "            FROM fact_race_results frr\n",
        "            JOIN dim_constructor dc ON frr.constructor_id = dc.constructor_id\n",
        "            WHERE frr.position = 1\n",
        "            GROUP BY frr.circuit_id, dc.constructor_name\n",
        "        )\n",
        "        SELECT\n",
        "            cs.*,\n",
        "            msd.driver_full_name as most_successful_driver,\n",
        "            msd.wins as driver_wins,\n",
        "            msc.constructor_name as most_successful_constructor,\n",
        "            msc.wins as constructor_wins\n",
        "        FROM circuit_stats cs\n",
        "        LEFT JOIN most_successful_drivers msd ON cs.circuit_id = msd.circuit_id AND msd.rank = 1\n",
        "        LEFT JOIN most_successful_constructors msc ON cs.circuit_id = msc.circuit_id AND msc.rank = 1\n",
        "        ORDER BY cs.total_races DESC\n",
        "    \"\"\")\n",
        "\n",
        "    # Write the view\n",
        "    circuit_statistics_df.write.mode(\"overwrite\").parquet(f\"{gold_path}/view_circuit_statistics\")\n",
        "    print(\"Circuit statistics view created\")\n",
        "\n",
        "def create_aggregated_views(spark, gold_path, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Create aggregated views for analytics\n",
        "    \"\"\"\n",
        "    # Get the last processed timestamp\n",
        "    last_processed_ts = get_last_processed_timestamp(spark, f\"{checkpoint_path}/aggregated_views\")\n",
        "    print(f\"Last processed timestamp for aggregated views: {last_processed_ts}\")\n",
        "\n",
        "    # Define paths\n",
        "    dim_driver_path = f\"{gold_path}/dim_driver\"\n",
        "    dim_constructor_path = f\"{gold_path}/dim_constructor\"\n",
        "    dim_circuit_path = f\"{gold_path}/dim_circuit\"\n",
        "    fact_race_results_path = f\"{gold_path}/fact_race_results\"\n",
        "    fact_driver_standings_path = f\"{gold_path}/fact_driver_standings\"\n",
        "    fact_constructor_standings_path = f\"{gold_path}/fact_constructor_standings\"\n",
        "\n",
        "    # Load dimension and fact tables\n",
        "    try:\n",
        "        dim_driver = spark.read.parquet(dim_driver_path)\n",
        "        dim_constructor = spark.read.parquet(dim_constructor_path)\n",
        "        dim_circuit = spark.read.parquet(dim_circuit_path)\n",
        "        fact_race_results = spark.read.parquet(fact_race_results_path)\n",
        "        fact_driver_standings = spark.read.parquet(fact_driver_standings_path)\n",
        "        fact_constructor_standings = spark.read.parquet(fact_constructor_standings_path)\n",
        "\n",
        "        # Register tables for SQL queries\n",
        "        dim_driver.filter(F.col(\"is_current\") == True).createOrReplaceTempView(\"dim_driver\")\n",
        "        dim_constructor.filter(F.col(\"is_current\") == True).createOrReplaceTempView(\"dim_constructor\")\n",
        "        dim_circuit.filter(F.col(\"is_current\") == True).createOrReplaceTempView(\"dim_circuit\")\n",
        "        fact_race_results.createOrReplaceTempView(\"fact_race_results\")\n",
        "        fact_driver_standings.createOrReplaceTempView(\"fact_driver_standings\")\n",
        "        fact_constructor_standings.createOrReplaceTempView(\"fact_constructor_standings\")\n",
        "\n",
        "        # Create season summary view\n",
        "        create_season_summary_view(spark, gold_path)\n",
        "\n",
        "        # Create driver performance view\n",
        "        create_driver_performance_view(spark, gold_path)\n",
        "\n",
        "        # Create constructor performance view\n",
        "        create_constructor_performance_view(spark, gold_path)\n",
        "\n",
        "        # Create circuit statistics view\n",
        "        create_circuit_statistics_view(spark, gold_path)\n",
        "\n",
        "        # Update checkpoint\n",
        "        update_checkpoint(spark, f\"{checkpoint_path}/aggregated_views\", datetime.now().isoformat())\n",
        "\n",
        "        print(\"Aggregated views creation complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating aggregated views: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def ensure_directories_exist(spark, paths):\n",
        "    \"\"\"\n",
        "    Create directories if they don't exist\n",
        "    \"\"\"\n",
        "    for path in paths:\n",
        "        try:\n",
        "            hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(path)\n",
        "            fs = hadoop_path.getFileSystem(spark._jsc.hadoopConfiguration())\n",
        "            if not fs.exists(hadoop_path):\n",
        "                fs.mkdirs(hadoop_path)\n",
        "                print(f\"Created directory: {path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating directory {path}: {str(e)}\")\n",
        "\n",
        "def process_gold_layer_incremental(silver_path, gold_path, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Main function to process the gold layer incrementally\n",
        "    \"\"\"\n",
        "    # Initialize Spark\n",
        "    spark = initialize_spark()\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    ensure_directories_exist(spark, [\n",
        "        gold_path,\n",
        "        checkpoint_path,\n",
        "        f\"{checkpoint_path}/dim_tables\",\n",
        "        f\"{checkpoint_path}/fact_tables\",\n",
        "        f\"{checkpoint_path}/aggregated_views\"\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        print(\"Starting Gold Layer Processing...\")\n",
        "\n",
        "        # Process dimension tables with SCD Type 2\n",
        "        print(\"\\\\nProcessing dimension tables...\")\n",
        "        process_dimension_tables(spark, silver_path, gold_path, checkpoint_path)\n",
        "\n",
        "        # Process fact tables incrementally\n",
        "        print(\"\\\\nProcessing fact tables...\")\n",
        "        process_fact_tables(spark, silver_path, gold_path, checkpoint_path)\n",
        "\n",
        "        # Create aggregated views\n",
        "        print(\"\\\\nCreating aggregated views...\")\n",
        "        create_aggregated_views(spark, gold_path, checkpoint_path)\n",
        "\n",
        "        print(\"\\\\nGold Layer Processing Complete!\")\n",
        "\n",
        "        # Show sample of processed data\n",
        "        print(\"\\\\nSample of dimension data:\")\n",
        "        spark.read.parquet(f\"{gold_path}/dim_driver\").filter(F.col(\"is_current\") == True).show(5)\n",
        "\n",
        "        print(\"\\\\nSample of fact data:\")\n",
        "        spark.read.parquet(f\"{gold_path}/fact_race_results\").filter(F.col(\"season\") == 2024).show(5)\n",
        "\n",
        "        print(\"\\\\nSample of aggregated view:\")\n",
        "        spark.read.parquet(f\"{gold_path}/view_season_summary\").show(5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Gold Layer Processing: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    silver_path = \"/content/drive/MyDrive/Capstone/silver\"\n",
        "    gold_path = \"/content/drive/MyDrive/Capstone/gold\"\n",
        "    checkpoint_path = \"/content/drive/MyDrive/Capstone/checkpoint/gold\"\n",
        "\n",
        "    # Execute the processing\n",
        "    process_gold_layer_incremental(silver_path, gold_path, checkpoint_path)"
      ]
    }
  ]
}